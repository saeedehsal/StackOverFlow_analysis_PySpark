{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "print sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import time\n",
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "source": [
    "# Spark for StackOverFlow analysis\n",
    "\n",
    "StackOverflow is a collaboratively edited question-and-answer site originally focused on programming topics. Because of the variety of features tracked, including a variety of feedback metrics, it allows for some open-ended analysis of user behavior on the site.\n",
    "\n",
    "StackExchange (the parent organization) provides an anonymized [data dump](https://archive.org/details/stackexchange), and we'll use Spark to perform data manipulation, analysis, and machine learning on this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Accessing the data\n",
    "\n",
    "The data is available on S3 (s3://dataincubator-course/spark-stack-data). There are three subfolders: allUsers, allPosts, and allVotes which contain chunked and gzipped xml with the following format:\n",
    "\n",
    "```\n",
    "<row Body=\"&lt;p&gt;I always validate my web pages, and I recommend you do the same BUT many large company websites DO NOT and cannot validate because the importance of the website looking exactly the same on all systems requires rules to be broken. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, valid websites help your page look good even on odd configurations (like cell phones) so you should always at least try to make it validate.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2008-10-12T20:26:29.397\" Id=\"195995\" LastActivityDate=\"2008-10-12T20:26:29.397\" OwnerDisplayName=\"Eric Wendelin\" OwnerUserId=\"25066\" ParentId=\"195973\" PostTypeId=\"2\" Score=\"0\" />\n",
    "```\n",
    "\n",
    "A full schema can be found [here](https://ia801500.us.archive.org/8/items/stackexchange/readme.txt).\n",
    "\n",
    "Data from the much smaller stats.stackexchange.com is available in the same format on S3 (s3://dataincubator-course/spark-stats-data). This site, Cross-Validated, will be used below in some instances to avoid working with the full dataset for every analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !mkdir -p spark-stats-data\n",
    "# !aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# !mkdir -p spark-stack-data\n",
    "# !aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stack-data/ ./spark-stack-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data input and parsing\n",
    "\n",
    "Some rows are split across multiple lines; these can be discarded. Malformatted XML can also be ignored. It is enough to simply skip problematic rows, the loss of data will not significantly impact our results on this large data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## bad_xml\n",
    "\n",
    "To test our parser function, we can first confirm that the number of bad XMLs (XML rows that started with ` <row` that were subsequently **rejected** during your processing) is 781."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bad xmls= 781\n"
     ]
    }
   ],
   "source": [
    "def localpath(path):\n",
    "    return  'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path\n",
    "\n",
    "posts = sc.textFile(localpath('spark-stats-data/allPosts/part-*.xml.gz'))\n",
    "totalLines = posts.count()\n",
    "\n",
    "def isInvalid(string):\n",
    "    try:\n",
    "        etree.fromstring(string)\n",
    "        return 0\n",
    "    except:\n",
    "        if '<row' in string:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "count = posts.map(isInvalid) \\\n",
    "    .reduce(lambda x, y: x+ y)\n",
    "    \n",
    "print 'number of bad xmls=', count   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## upvote_percentage\n",
    "\n",
    "Each post on StackExchange can be upvoted, downvoted, and favorited. One \"sanity check\" we can do is to look at the ratio of upvotes to downvotes (referred to as \"UpMod\" and \"DownMod\" in the schema) as a function of how many times the post has been favorited.\n",
    "\n",
    "We can hypothesize, for example, that posts with more favorites should have a higher upvote/downvote ratio.\n",
    "\n",
    "Instead of looking at individual posts, we'll aggregate across number of favorites by using the post's number of favorites as our key. Since we're computing ratios, bundling together all posts with the same number of favorites effectively averages over them.  Following is the average percentage of upvotes *(upvotes / (upvotes + downvotes))* for the first 50 ***keys***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "posts = sc.textFile(localpath('spark-stats-data/allVotes/part-*.xml.gz'))\n",
    "totalLines = posts.count()\n",
    "\n",
    "def getXmlInfo(string):\n",
    "    try:\n",
    "        row = etree.fromstring(string)\n",
    "        pid = row.attrib['PostId']\n",
    "        vid = row.attrib['VoteTypeId']\n",
    "        return (pid, vid)\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "def favupdown(vote):\n",
    "    postid, vid = vote\n",
    "    n_fav = 0\n",
    "    n_up = 0\n",
    "    n_down = 0\n",
    "    for v in vid : \n",
    "        if v == '5':\n",
    "            n_fav += 1\n",
    "        if v == '2':\n",
    "            n_up += 1\n",
    "        if v == '3' :\n",
    "            n_down += 1\n",
    "    return (n_fav, (n_up, n_down))\n",
    "\n",
    "def percentage(vote) :\n",
    "    per = float(vote[1][0])/(vote[1][1] + vote[1][0])\n",
    "    return (vote[0], per)\n",
    "        \n",
    "count = posts.map(getXmlInfo) \\\n",
    "        .filter(lambda x : x)\\\n",
    "        .groupByKey() \\\n",
    "        .map(favupdown) \\\n",
    "        .reduceByKey(lambda x, y :(x[0] + y[0], x[1] + y[1])) \\\n",
    "        .sortByKey() \\\n",
    "        .map(percentage) \\\n",
    "        .collect() \n",
    "q1 = count[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9515184306202837), (1, 0.971349277609991), (2, 0.9858878575201871), (3, 0.9899873257287706), (4, 0.990321980271729), (5, 0.9925945517058979), (6, 0.9948542024013722), (7, 0.9908026755852842), (8, 0.9944289693593314), (9, 0.9967931587386424), (10, 0.9916376306620209), (11, 0.9915174363807728), (12, 0.9958123953098827), (13, 0.9972789115646259), (14, 0.9939540507859734), (15, 0.9929245283018868), (16, 1.0), (17, 1.0), (18, 0.9985693848354793), (19, 0.997867803837953), (20, 0.9969512195121951), (21, 0.9944029850746269), (22, 0.9977973568281938), (23, 0.9952038369304557), (24, 1.0), (25, 1.0), (26, 0.9841772151898734), (27, 0.989010989010989), (28, 0.9951690821256038), (29, 0.9972826086956522), (30, 0.9954337899543378), (31, 0.9939577039274925), (32, 1.0), (33, 1.0), (34, 1.0), (35, 1.0), (36, 1.0), (37, 0.990990990990991), (38, 0.9937888198757764), (39, 0.9918032786885246), (40, 1.0), (41, 1.0), (42, 1.0), (44, 1.0), (45, 1.0), (47, 1.0), (48, 1.0), (49, 1.0), (50, 1.0), (52, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## answer_percentage\n",
    "\n",
    "In this part we try to investigate the correlation between a user's reputation and the kind of posts they make.\n",
    "\n",
    "For the 99 users with the highest reputation, we can look at the percentage of these posts that are answers: *(answers / (answers + questions))*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class user_class(object):\n",
    "    def __init__(self, Id, Rep):\n",
    "        self.Id = Id\n",
    "        self.Rep = Rep\n",
    "\n",
    "class post_class(object):\n",
    "    def __init__(self, Id, qa):\n",
    "        self.Id = Id\n",
    "        self.qa = qa\n",
    "        \n",
    "\n",
    "def construct_user(string):\n",
    "    try:\n",
    "        row = etree.fromstring(string)\n",
    "        user_id = row.attrib['Id']\n",
    "        user_rep = row.attrib['Reputation']\n",
    "        return user_class(user_id, user_rep)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def construct_post(string):\n",
    "    try:\n",
    "        row = etree.fromstring(string)\n",
    "        user_id = row.attrib['OwnerUserId']\n",
    "        qa = row.attrib['PostTypeId']\n",
    "        return post_class(user_id, qa)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def numqa(tup):\n",
    "    user_id, list1  = tup\n",
    "    n_q = 0\n",
    "    n_a = 0\n",
    "    rep = 0\n",
    "    for list11 in list1 : \n",
    "        if list11[1]  == '1':\n",
    "            n_q += 1\n",
    "            rep = list11[0]\n",
    "        elif list11[1] == '2' :\n",
    "            n_a += 1\n",
    "            rep = list11[0]\n",
    "    return (user_id, rep, (n_a, n_q))\n",
    "\n",
    "\n",
    "users = sc.textFile(localpath('spark-stats-data/allUsers/part-*.xml.gz')).map(construct_user)\n",
    "posts = sc.textFile(localpath('spark-stats-data/allPosts/part-*.xml.gz')).map(construct_post) \n",
    "\n",
    "q3 = users.filter(lambda x: x is not None).map(lambda u : (u.Id, u.Rep)) \\\n",
    ".join(posts.filter(lambda x: x is not None).map(lambda p : (p.Id, p.qa))) \\\n",
    ".groupByKey() \\\n",
    ".map(numqa) \\\n",
    ".filter(lambda x: x[2][1] != 0) \\\n",
    ".map(lambda t: (int(t[1]), (t[0], float(t[2][0]) / (t[2][0] + t[2][1]))))\\\n",
    ".sortByKey(False)\\\n",
    ".map(lambda t: (int(t[1][0]), t[1][1])) \\\n",
    ".collect()\n",
    "\n",
    "q3_final = q3[:99]\n",
    "q3_final.append((-1,0.19991701720554747))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(919, 0.996694214876033), (805, 0.9959749552772809), (686, 0.9803049555273189), (7290, 0.9918887601390498), (930, 0.9817351598173516), (4253, 0.9909747292418772), (183, 0.847870182555781), (11032, 0.9875647668393782), (28746, 0.968421052631579), (887, 0.9794871794871794), (159, 0.9728813559322034), (2116, 0.9833333333333333), (4856, 0.9543147208121827), (5739, 0.9872773536895675), (3277, 0.956081081081081), (88, 0.9660493827160493), (601, 0.9772151898734177), (17230, 0.9970059880239521), (2392, 0.9724137931034482), (1390, 0.9411764705882353), (5836, 0.846441947565543), (603, 0.8158844765342961), (7972, 0.9823008849557522), (6633, 0.9912280701754386), (2958, 0.9930313588850174), (9394, 0.9700854700854701), (7828, 0.9850427350427351), (2817, 0.8206896551724138), (7224, 0.9757575757575757), (4598, 0.9857142857142858), (7071, 0.9107142857142857), (1739, 0.9948717948717949), (1036, 0.9545454545454546), (8013, 0.9040697674418605), (3019, 0.8571428571428571), (4376, 0.963302752293578), (251, 0.9924242424242424), (28666, 0.9), (1764, 0.9325842696629213), (32036, 0.9959839357429718), (10849, 0.9518072289156626), (26338, 0.9691358024691358), (1352, 0.9902912621359223), (401, 0.9119496855345912), (5, 0.8547008547008547), (8, 0.8991596638655462), (7250, 0.9877300613496932), (1909, 0.9518072289156626), (21054, 0.9345794392523364), (4257, 0.9757575757575757), (196, 0.7357512953367875), (442, 0.8712121212121212), (2669, 0.946843853820598), (8402, 0.6521739130434783), (36041, 0.9889807162534435), (44269, 0.9033613445378151), (11981, 0.9649122807017544), (1934, 0.9680851063829787), (795, 0.676056338028169), (25433, 0.9867256637168141), (253, 0.3695652173913043), (364, 0.6736111111111112), (25, 0.9166666666666666), (22311, 0.9401709401709402), (13047, 0.9733333333333334), (8507, 0.9428571428571428), (264, 0.90625), (14188, 0.8983050847457628), (8076, 0.9333333333333333), (8413, 0.9836065573770492), (1307, 0.8333333333333334), (2860, 0.8903225806451613), (223, 0.8588235294117647), (11887, 0.976878612716763), (52554, 0.9652777777777778), (35989, 0.9487179487179487), (1005, 0.0034482758620689655), (22228, 0.8513513513513513), (4862, 0.8974358974358975), (3601, 0.9852941176470589), (13138, 0.8639455782312925), (1108, 0.9722222222222222), (1679, 0.941747572815534), (8373, 0.9583333333333334), (1191, 0.9875), (2129, 0.9838709677419355), (2645, 0.9310344827586207), (1381, 0.5795454545454546), (8141, 0.9166666666666666), (858, 0.9145299145299145), (11867, 0.7654320987654321), (1569, 0.8829787234042553), (8451, 0.9830508474576272), (7365, 0.7808219178082192), (25212, 0.984375), (26, 0.9137931034482759), (5829, 0.9333333333333333), (1124, 0.8695652173913043), (339, 0.7049180327868853), (-1, 0.19991701720554747)]\n"
     ]
    }
   ],
   "source": [
    "print q3_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## post_counts\n",
    "\n",
    "If we use the total number of posts made on the site as a metric for tenure, we can look at the differences between \"younger\" and \"older\" users. In this part we return the top 100 post counts among all users (of all types of posts) and the average reputation for every user who has that count.\n",
    "\n",
    "In other words, we are aggregating the cases where multiple users have the same post count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2325, 92624.0), (1663, 47334.0), (1287, 100976.0), (1018, 46907.0), (965, 23102.0), (695, 27599.0), (570, 22706.0), (558, 25406.0), (495, 9294.0), (494, 23610.0), (469, 10728.0), (452, 32283.0), (424, 16854.0), (419, 17719.0), (395, 14100.0), (390, 20315.0), (369, 19312.0), (363, 6149.0), (350, 9047.0), (345, 14768.0), (343, 13557.0), (339, 11795.0), (338, 10045.0), (304, 16131.0), (301, 6352.0), (297, 20133.0), (292, 10552.0), (290, 8285.5), (287, 11083.0), (282, 10383.0), (277, 11830.0), (269, 7729.0), (268, 11989.0), (267, 7971.0), (265, 7765.0), (257, 13078.0), (248, 7608.0), (247, 12496.5), (239, 1.0), (234, 11307.5), (228, 11662.0), (226, 5775.0), (218, 5849.0), (211, 7552.0), (208, 6208.0), (202, 9530.0), (195, 9619.0), (193, 6682.0), (188, 12098.0), (187, 8013.0), (185, 4149.0), (184, 5762.0), (177, 5042.0), (173, 10394.0), (168, 7725.0), (167, 3957.0), (165, 6694.0), (164, 1544.0), (163, 6888.0), (161, 6367.0), (159, 7116.0), (157, 6040.0), (156, 4086.6666666666665), (155, 4204.0), (150, 5398.0), (147, 3821.0), (146, 4127.0), (145, 2189.0), (144, 4943.0), (140, 1063.0), (133, 8794.0), (132, 7404.5), (131, 1875.0), (128, 5085.0), (124, 3650.0), (122, 2401.0), (119, 6948.0), (118, 3736.5), (117, 5237.0), (114, 5970.0), (113, 1267.0), (112, 2052.5), (111, 2533.0), (110, 2992.0), (109, 8629.0), (107, 6430.5), (105, 3890.0), (103, 3747.0), (101, 2597.0), (99, 2465.3333333333335), (98, 1584.0), (96, 2336.0), (94, 4668.0), (90, 3315.5), (88, 3036.5), (87, 2110.0), (86, 1282.0), (85, 2054.3333333333335), (84, 3880.5), (83, 3237.5)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class user_class(object):\n",
    "    def __init__(self, Id, Rep):\n",
    "        self.Id = Id\n",
    "        self.Rep = Rep\n",
    "\n",
    "class post_class(object):\n",
    "    def __init__(self, Id, qa):\n",
    "        self.Id = Id\n",
    "        self.qa = qa\n",
    "\n",
    "def construct_user(string):\n",
    "    try:\n",
    "        row = etree.fromstring(string)\n",
    "        user_id = row.attrib['Id']\n",
    "        user_rep = row.attrib['Reputation']\n",
    "        return user_class(user_id, user_rep)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def construct_post(string):\n",
    "    try:\n",
    "        row = etree.fromstring(string)\n",
    "        user_id = row.attrib['OwnerUserId']\n",
    "        qa = row.attrib['PostTypeId']\n",
    "        return post_class(user_id, qa)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def num_post(tup):\n",
    "    user_id, list1  = tup\n",
    "    num_post = 0\n",
    "    for list11 in list1 : \n",
    "        num_post += 1\n",
    "        rep = list11[0]\n",
    "    return (num_post, int(rep))\n",
    "\n",
    "def rep_avg(tup):\n",
    "    num_post , rep = tup\n",
    "    cnt = 0\n",
    "    total_rep = 0\n",
    "    for el in rep:\n",
    "        cnt += 1\n",
    "        total_rep += el\n",
    "    return (num_post, total_rep/float(cnt))\n",
    "        \n",
    "    \n",
    "\n",
    "users = sc.textFile(localpath('spark-stats-data/allUsers/part-*.xml.gz')).map(construct_user)\n",
    "posts = sc.textFile(localpath('spark-stats-data/allPosts/part-*.xml.gz')).map(construct_post) \n",
    "\n",
    "q4 = users.filter(lambda x: x is not None).map(lambda u : (u.Id, u.Rep)) \\\n",
    "    .join(posts.filter(lambda x: x is not None).map(lambda p : (p.Id, p.qa))) \\\n",
    "    .groupByKey() \\\n",
    "    .map(num_post) \\\n",
    "    .groupByKey() \\\n",
    "    .map(rep_avg) \\\n",
    "    .sortByKey(False) \\\n",
    "    .collect()\n",
    "    \n",
    "print q4[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## quick_answers\n",
    "\n",
    "How long do we have to wait to get your question answered? In this part, we look at the set of ACCEPTED answers which are posted less than three hours after question creation. we are trying to answer some of the following questions:\n",
    "\n",
    "What is the average number of these \"quick answers\" as a function of the hour of day the question was asked? We should normalize by how many total accepted answers are garnered by questions posted in a given hour, just like we're counting how many quick accepted answers are garnered by questions posted in a given hour, eg. (quick accepted answers when question hour is 15 / total accepted answers when question hour is 15).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def localpath(path):\n",
    "    return  'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path\n",
    "\n",
    "posts = sc.textFile(localpath('spark-stats-data/allPosts/part-*.xml.gz'))\n",
    "\n",
    "class qa_class(object):\n",
    "    def __init__(self, acc_id, cre_date):\n",
    "        self.id = acc_id\n",
    "        self.date = cre_date\n",
    "\n",
    "def construct_qa(string):\n",
    "    try:\n",
    "        row = etree.fromstring(string)\n",
    "        cre_date = parse(row.attrib['CreationDate'])\n",
    "        acc_id = row.attrib['AcceptedAnswerId']\n",
    "        return qa_class(acc_id, cre_date)\n",
    "        #return acc_id, cre_Date\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "class ans_class(object):\n",
    "    def __init__(self, ans_id, cre_date):\n",
    "        self.id = ans_id\n",
    "        self.date = cre_date\n",
    "\n",
    "def construct_ans(string):\n",
    "    try:\n",
    "        row = etree.fromstring(string)\n",
    "        cre_date = parse(row.attrib['CreationDate'])\n",
    "        ans_id = row.attrib['Id']\n",
    "        return ans_class(ans_id, cre_date)\n",
    "        #return ans_id, cre_date\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def avg_def_hour(tup):\n",
    "    h, diff = tup\n",
    "    n = 0\n",
    "    n_valid = 0\n",
    "    for d in diff :\n",
    "        n += 1\n",
    "        if int(d) < 10800 :\n",
    "            n_valid += 1\n",
    "    return (h, float(n_valid)/n)\n",
    "    \n",
    "q5 = posts.map(construct_qa).filter(lambda x: x is not None).map(lambda u : (u.id, u.date))\\\n",
    "    .join(posts.map(construct_ans).filter(lambda x: x is not None).map(lambda u: (u.id, u.date))) \\\n",
    "    .map(lambda t: (t[1][0].hour , (t[1][1] - t[1][0]).total_seconds())) \\\n",
    "    .groupByKey()\\\n",
    "    .map(avg_def_hour)\\\n",
    "    .sortByKey()\\\n",
    "    .map(lambda t: t[1])\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4504672897196262, 0.44814814814814813, 0.3605577689243028, 0.3799126637554585, 0.4028436018957346, 0.4125, 0.4597402597402597, 0.4673684210526316, 0.4616822429906542, 0.49528301886792453, 0.5157593123209169, 0.5445682451253482, 0.5347313237221494, 0.5310796074154853, 0.5238095238095238, 0.5368007850834151, 0.5475728155339806, 0.47995991983967934, 0.5202185792349727, 0.5462012320328542, 0.5196408529741863, 0.5156794425087108, 0.46153846153846156, 0.4700460829493088]\n"
     ]
    }
   ],
   "source": [
    "print q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## identify_veterans\n",
    "\n",
    "It can be interesting to think about what factors influence a user to remain active on the site over a long period of time. In order not to bias the results towards older users, we'll define a time window between 100 and 150 days after account creation. If the user has made a post in this time, we'll consider them active and well on their way to being veterans of the site; if not, they are inactive and were likely brief users.\n",
    "\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of \"veterans\" vs. \"brief users\". For each group separately, we find the average of the score, views, number of answers, and number of favorites of the users' **first question**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def localpath(path):\n",
    "    return  'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path\n",
    "\n",
    "posts = sc.textFile(localpath('spark-stats-data/allPosts/part-*.xml.gz'))\n",
    "users = sc.textFile(localpath('spark-stats-data/allUsers/part-*.xml.gz'))\n",
    "\n",
    "class user_class(object):\n",
    "    def __init__(self, user_id, cre_date):\n",
    "        self.id = user_id\n",
    "        self.cre_date = cre_date\n",
    "\n",
    "def construct_user(string):\n",
    "    try:\n",
    "        row = etree.fromstring(string)\n",
    "        cre_date = parse(row.attrib['CreationDate'])\n",
    "        user_id = row.attrib['Id']\n",
    "        return user_class(user_id, cre_date)\n",
    "        #return acc_id, cre_Date\n",
    "    except:\n",
    "        return None\n",
    "       \n",
    "class post_class(object):\n",
    "    def __init__(self, user_id, cre_date, view, score, fav_cnt, ans_cnt, post_type_id):\n",
    "        self.id = user_id\n",
    "        self.date = cre_date\n",
    "        self.vw = view\n",
    "        self.scr = score\n",
    "        self.fav= fav_cnt\n",
    "        self.ans = ans_cnt\n",
    "        self.post = post_type_id\n",
    "\n",
    "def construct_post(string):\n",
    "    try:\n",
    "        row = etree.fromstring(string)\n",
    "        user_id = row.attrib['OwnerUserId']\n",
    "        cre_date = parse(row.attrib['CreationDate'])\n",
    "        try:\n",
    "            view = row.attrib['ViewCount']\n",
    "        except:\n",
    "            view = 0\n",
    "        try:\n",
    "            score = row.attrib['Score']\n",
    "        except:\n",
    "            score = 0\n",
    "        try:\n",
    "            fav_cnt = row.attrib['FavoriteCount']\n",
    "        except:\n",
    "            fav_cnt = 0\n",
    "        try:\n",
    "            ans_cnt = row.attrib['AnswerCount']\n",
    "        except:\n",
    "            ans_cnt = 0\n",
    "        post_type_id = row.attrib['PostTypeId']           \n",
    "        return post_class(user_id, cre_date, view, score, fav_cnt, ans_cnt, post_type_id)\n",
    "        #return user_id, cre_date, view, score, fav_cnt, ans_cnt, post_type_id\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def is_veteran1(tup):\n",
    "    user, list1 = tup\n",
    "    is_veteran = False   \n",
    "    for l in list1:\n",
    "        is_veteran = (is_veteran) or ((l[0] > 100) and (l[0] < 150))\n",
    "    return (user, is_veteran)\n",
    "\n",
    "def first_q(tup):\n",
    "    user, his_posts = tup\n",
    "    first_post = min(his_posts, key = lambda t: t[1][0])\n",
    "    return user, first_post\n",
    "\n",
    "def avg_vals(tup):\n",
    "    vet, list1 = tup\n",
    "    n = 0\n",
    "    view_total = 0\n",
    "    score_total = 0\n",
    "    fav_cnt = 0\n",
    "    ans_cnt = 0\n",
    "    for list11 in list1:\n",
    "        n = n + 1\n",
    "        view_total += int(list11[0])\n",
    "        score_total += int(list11[1])\n",
    "        fav_cnt += int(list11[2])\n",
    "        ans_cnt += int(list11[3])\n",
    "    return (vet, float(view_total)/n, float(score_total)/n, float(fav_cnt)/n, float(ans_cnt)/n)\n",
    "               \n",
    "q7 = posts.map(construct_post).filter(lambda x: x is not None) \\\n",
    ".map(lambda u : (u.id, (u.date, u.vw, u.scr, u.fav, u.ans, u.post))) \\\n",
    ".join(users.map(construct_user).filter(lambda x: x is not None).map(lambda t: (t.id, (t.cre_date))))\\\n",
    ".map(lambda t: (t[0], ((t[1][0][0]-t[1][1]).days, t[1][0][1], t[1][0][2], t[1][0][3], t[1][0][4], t[1][0][5])))\\\n",
    ".groupByKey().map(is_veteran1).join(posts.map(construct_post).filter(lambda x: x is not None)\\\n",
    ".map(lambda u : (u.id, (u.date, u.vw, u.scr, u.fav, u.ans, u.post)))).filter(lambda x: x[1][1][5] == '1')\\\n",
    ".groupByKey().map(first_q).map(lambda t: (t[1][0], (t[1][1][1], t[1][1][2], t[1][1][3], t[1][1][4])))\\\n",
    ".groupByKey().map(avg_vals).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brief_answers': 2.1031764650682683,\n",
       " 'brief_favorites': 0.5773002392905738,\n",
       " 'brief_score': 553.5367616009008,\n",
       " 'brief_views': False,\n",
       " 'vet_answers': 3.5443322109988777,\n",
       " 'vet_favorites': 1.2985409652076318,\n",
       " 'vet_score': 933.7210998877665,\n",
       " 'vet_views': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identify_veterans():\n",
    "    return {\"vet_score\": q7[1][1],\n",
    "            \"vet_views\": q7[1][0],\n",
    "            \"vet_answers\": q7[1][2],\n",
    "            \"vet_favorites\": q7[1][3],\n",
    "            \"brief_score\": q7[0][1] ,\n",
    "            \"brief_views\": q7[0][0],\n",
    "            \"brief_answers\": q7[0][2],\n",
    "            \"brief_favorites\": q7[0][3]\n",
    "           }\n",
    "\n",
    "identify_veterans()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## word2vec\n",
    "\n",
    "Word2Vec is an alternative approach for vectorizing text data. The vectorized representations of words in the vocabulary tend to be useful for predicting other words in the document, hence the famous example \"vector('king') - vector('man') + vector('woman') ~= vector('queen')\".\n",
    "\n",
    "Let's see how good a Word2Vec model we can train using the tags of each StackExchange post as documents (this uses the full dataset). We will use Spark ML's implementation of Word2Vec (this will require using DataFrames) to return a list of the top 25 closest synonyms to \"ggplot2\" and their similarity score in tuple format (\"string\", number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'lattice', 0.92138660954231466), (u'r-grid', 0.85115660268066085), (u'plotrix', 0.84942440168473787), (u'boxplot', 0.8468789078740947), (u'plotmath', 0.84554861178583363), (u'ecdf', 0.83877705555707738), (u'levelplot', 0.83423389393602954), (u'line-plot', 0.83005077357219181), (u'loess', 0.82561341374355501), (u'density-plot', 0.82245318334253437), (u'gridextra', 0.82094705140050261), (u'quantile', 0.81995369147794173), (u'melt', 0.8163310299767258), (u'standard-error', 0.81067610439807858), (u'categorical-data', 0.80948906895083605), (u'tapply', 0.80768296238242443), (u'r-factor', 0.80725529343464109), (u'performanceanalytics', 0.80423577122124168), (u'rgl', 0.80418713447199341), (u'ggvis', 0.80172868913410533), (u'mgcv', 0.80078921808747217), (u'survival-analysis', 0.80028066752686966), (u'kernel-density', 0.79897911880171124), (u'anova', 0.79872824324163849), (u'geom-text', 0.79759711080469742)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "import re\n",
    "\n",
    "def localpath(path):\n",
    "    return  'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path\n",
    "\n",
    "def valid_xml(string):\n",
    "    if '<row' in string  and '/>' in string :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def extracttags(string):\n",
    "    try:\n",
    "        tree = etree.fromstring(string)\n",
    "        tags = tree.get(\"Tags\")\n",
    "        tags = tags[1:-1].split('><')\n",
    "        return tags\n",
    "    except: \n",
    "        return None\n",
    "\n",
    "posts_complete = sc.textFile(localpath('spark-stack-data/allPosts/part-*.xml.gz'))\n",
    "q8 = posts_complete.filter(valid_xml)\\\n",
    "    .map(extracttags)\\\n",
    "    .filter(lambda x:x is not None)\n",
    "    \n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(q8)\n",
    "\n",
    "synonyms = model.findSynonyms('ggplot2', 25)\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
